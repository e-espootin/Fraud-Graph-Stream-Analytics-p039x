{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107dc70a-cbc7-459a-9b80-6d894a3f6dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e7a0e4-a327-4307-babf-da741a101804",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StreamToTable\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4,org.apache.kafka:kafka-clients:3.2.0,org.postgresql:postgresql:42.7.4,com.datastax.spark:spark-cassandra-connector_2.13:3.5.1,ru.yandex.clickhouse:clickhouse-jdbc:0.3.2\") \\\n",
    "    .getOrCreate()\n",
    "    #.config(\"spark.cassandra.connection.host\", \"scylla\") \\\n",
    "    #.config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df88b3d5-63f3-4089-bede-0f9b5bd4ee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.4\n",
      "Spark JARs: ArraySeq(file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.4.jar, file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.7.4.jar, file:///home/jovyan/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.13-3.5.1.jar, file:///home/jovyan/.ivy2/jars/ru.yandex.clickhouse_clickhouse-jdbc-0.3.2.jar, file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.4.jar, file:///home/jovyan/.ivy2/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar, file:///home/jovyan/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar, file:///home/jovyan/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar, file:///home/jovyan/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar, file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar, file:///home/jovyan/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar, file:///home/jovyan/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.5.jar, file:///home/jovyan/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar, file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar, file:///home/jovyan/.ivy2/jars/org.checkerframework_checker-qual-3.42.0.jar, file:///home/jovyan/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.13-3.5.1.jar, file:///home/jovyan/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.13-2.11.0.jar, file:///home/jovyan/.ivy2/jars/org.apache.cassandra_java-driver-core-shaded-4.18.1.jar, file:///home/jovyan/.ivy2/jars/org.apache.cassandra_java-driver-mapper-runtime-4.18.1.jar, file:///home/jovyan/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar, file:///home/jovyan/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar, file:///home/jovyan/.ivy2/jars/org.scala-lang_scala-reflect-2.13.13.jar, file:///home/jovyan/.ivy2/jars/com.datastax.oss_native-protocol-1.5.1.jar, file:///home/jovyan/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar, file:///home/jovyan/.ivy2/jars/com.typesafe_config-1.4.1.jar, file:///home/jovyan/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar, file:///home/jovyan/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar, file:///home/jovyan/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar, file:///home/jovyan/.ivy2/jars/org.apache.cassandra_java-driver-query-builder-4.18.1.jar, file:///home/jovyan/.ivy2/jars/com.clickhouse_clickhouse-http-client-0.3.2.jar, file:///home/jovyan/.ivy2/jars/com.google.code.gson_gson-2.8.8.jar, file:///home/jovyan/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.13.jar, file:///home/jovyan/.ivy2/jars/org.apache.httpcomponents_httpmime-4.5.13.jar, file:///home/jovyan/.ivy2/jars/com.clickhouse_clickhouse-client-0.3.2.jar, file:///home/jovyan/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.13.jar, file:///home/jovyan/.ivy2/jars/commons-logging_commons-logging-1.2.jar, file:///home/jovyan/.ivy2/jars/commons-codec_commons-codec-1.11.jar)\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Spark JARs:\", spark.sparkContext._jsc.sc().jars())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a167d38b-e8c8-4c97-90b4-d0558631400e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scala Version: version 2.13.8\n"
     ]
    }
   ],
   "source": [
    "scala_version = spark.sparkContext._gateway.jvm.scala.util.Properties.versionString()\n",
    "print(\"Scala Version:\", scala_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "041bf3da-f3ec-4fd2-9262-7f6d5abd63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to write each batch of data to PostgreSQL\n",
    "def write_to_postgres(batch_df, batch_id):\n",
    "    batch_df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:postgresql://postgres:5432/mydatabase\") \\\n",
    "        .option(\"dbtable\", \"t1\") \\\n",
    "        .option(\"user\", \"postgres\") \\\n",
    "        .option(\"password\", \"postgres\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3df46e4-38da-44c5-9710-591f40f5e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_scylladb(batch_df, batch_id):\n",
    "    try:\n",
    "        print(f\"Writing batch {batch_id} to ScyllaDB\")\n",
    "        batch_df.write \\\n",
    "            .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "            .options(keyspace=\"keyspace_dev\", table=\"fin_trans_table\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "        print(f\"Batch {batch_id} written successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing batch {batch_id} to ScyllaDB: {e}\")\n",
    "        raise e  # Fail the stream if the batch fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "243e93f1-46d7-49cc-ba0a-c50708a6f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write micro-batches to ClickHouse\n",
    "def write_to_clickhouse(batch_df, batch_id):\n",
    "    try:\n",
    "        print(f\"Writing batch {batch_id} to ClickHouse\")\n",
    "        batch_df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:clickhouse://clickhouse:8123/testdb1\") \\\n",
    "            .option(\"dbtable\", \"testdb1.fin_trans_table\") \\\n",
    "            .option(\"user\", \"click\") \\\n",
    "            .option(\"password\", \"click\") \\\n",
    "            .option(\"driver\", \"ru.yandex.clickhouse.ClickHouseDriver\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "        print(f\"Batch {batch_id} written successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing batch {batch_id} to ClickHouse: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "702ba932-50b2-4097-bb67-77a0eb2c7ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- kafka_key: string (nullable = true)\n",
      " |-- kafka_value: string (nullable = true)\n",
      "\n",
      "Writing batch 1065 to ClickHouse\n",
      "Batch 1065 written successfully\n",
      "Writing batch 1066 to ClickHouse\n",
      "Batch 1066 written successfully\n",
      "Writing batch 1067 to ClickHouse\n",
      "Batch 1067 written successfully\n",
      "Writing batch 1068 to ClickHouse\n",
      "Batch 1068 written successfully\n",
      "Writing batch 1069 to ClickHouse\n",
      "Batch 1069 written successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Write the streaming data to PostgreSQL using foreachBatch\u001b[39;00m\n\u001b[1;32m     25\u001b[0m query \u001b[38;5;241m=\u001b[39m query_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(write_to_clickhouse) \\\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jovyan/work/_spark_metadata/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 31\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing batch 1070 to ClickHouse\n",
      "Batch 1070 written successfully\n",
      "Writing batch 1071 to ClickHouse\n",
      "Batch 1071 written successfully\n",
      "Writing batch 1072 to ClickHouse\n",
      "Batch 1072 written successfully\n",
      "Writing batch 1073 to ClickHouse\n",
      "Batch 1073 written successfully\n",
      "Writing batch 1074 to ClickHouse\n",
      "Batch 1074 written successfully\n",
      "Writing batch 1075 to ClickHouse\n",
      "Batch 1075 written successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark\\\n",
    "      .readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "      .option(\"subscribe\", \"fin_trans_topic\") \\\n",
    "      .option(\"startingOffsets\", \"earliest\") \\\n",
    "      .option(\"failOnDataLoss\", \"false\") \\\n",
    "      .load()\n",
    "      \n",
    "'''\n",
    "query = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"checkpointLocation\", \"/home/jovyan/work/book_data/\") \\\n",
    "    .start()\n",
    "'''\n",
    "\n",
    "# Select and cast key and value columns to string\n",
    "query_df = df.selectExpr(\"CAST(key AS STRING) AS kafka_key\", \"CAST(value AS STRING) AS kafka_value\")\n",
    "\n",
    "query_df.printSchema()\n",
    "\n",
    "\n",
    "# Write the streaming data to PostgreSQL using foreachBatch\n",
    "query = query_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(write_to_clickhouse) \\\n",
    "    .option(\"checkpointLocation\", \"/home/jovyan/work/_spark_metadata/\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a856aa5-b42c-4c9f-b4a1-2c7fdd47c19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1524a30-991f-4766-9f40-4e378cc83986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
