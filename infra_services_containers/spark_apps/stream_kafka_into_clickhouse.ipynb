{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107dc70a-cbc7-459a-9b80-6d894a3f6dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e7a0e4-a327-4307-babf-da741a101804",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StreamToTable\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4,org.apache.kafka:kafka-clients:3.2.0,org.postgresql:postgresql:42.7.4,com.datastax.spark:spark-cassandra-connector_2.13:3.5.1,ru.yandex.clickhouse:clickhouse-jdbc:0.3.2\") \\\n",
    "    .getOrCreate()\n",
    "    #.config(\"spark.cassandra.connection.host\", \"scylla\") \\\n",
    "    #.config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df88b3d5-63f3-4089-bede-0f9b5bd4ee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.4\n",
      "Spark JARs: ArraySeq(file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.4.jar, file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.7.4.jar, file:///home/jovyan/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.13-3.5.1.jar, file:///home/jovyan/.ivy2/jars/ru.yandex.clickhouse_clickhouse-jdbc-0.3.2.jar, file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.4.jar, file:///home/jovyan/.ivy2/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar, file:///home/jovyan/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar, file:///home/jovyan/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar, file:///home/jovyan/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar, file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar, file:///home/jovyan/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar, file:///home/jovyan/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.5.jar, file:///home/jovyan/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar, file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar, file:///home/jovyan/.ivy2/jars/org.checkerframework_checker-qual-3.42.0.jar, file:///home/jovyan/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.13-3.5.1.jar, file:///home/jovyan/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.13-2.11.0.jar, file:///home/jovyan/.ivy2/jars/org.apache.cassandra_java-driver-core-shaded-4.18.1.jar, file:///home/jovyan/.ivy2/jars/org.apache.cassandra_java-driver-mapper-runtime-4.18.1.jar, file:///home/jovyan/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar, file:///home/jovyan/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar, file:///home/jovyan/.ivy2/jars/org.scala-lang_scala-reflect-2.13.13.jar, file:///home/jovyan/.ivy2/jars/com.datastax.oss_native-protocol-1.5.1.jar, file:///home/jovyan/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar, file:///home/jovyan/.ivy2/jars/com.typesafe_config-1.4.1.jar, file:///home/jovyan/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar, file:///home/jovyan/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar, file:///home/jovyan/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar, file:///home/jovyan/.ivy2/jars/org.apache.cassandra_java-driver-query-builder-4.18.1.jar, file:///home/jovyan/.ivy2/jars/com.clickhouse_clickhouse-http-client-0.3.2.jar, file:///home/jovyan/.ivy2/jars/com.google.code.gson_gson-2.8.8.jar, file:///home/jovyan/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.13.jar, file:///home/jovyan/.ivy2/jars/org.apache.httpcomponents_httpmime-4.5.13.jar, file:///home/jovyan/.ivy2/jars/com.clickhouse_clickhouse-client-0.3.2.jar, file:///home/jovyan/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.13.jar, file:///home/jovyan/.ivy2/jars/commons-logging_commons-logging-1.2.jar, file:///home/jovyan/.ivy2/jars/commons-codec_commons-codec-1.11.jar)\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Spark JARs:\", spark.sparkContext._jsc.sc().jars())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a167d38b-e8c8-4c97-90b4-d0558631400e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scala Version: version 2.13.8\n"
     ]
    }
   ],
   "source": [
    "scala_version = spark.sparkContext._gateway.jvm.scala.util.Properties.versionString()\n",
    "print(\"Scala Version:\", scala_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "041bf3da-f3ec-4fd2-9262-7f6d5abd63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to write each batch of data to PostgreSQL\n",
    "def write_to_postgres(batch_df, batch_id):\n",
    "    batch_df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:postgresql://postgres:5432/mydatabase\") \\\n",
    "        .option(\"dbtable\", \"t1\") \\\n",
    "        .option(\"user\", \"postgres\") \\\n",
    "        .option(\"password\", \"postgres\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3df46e4-38da-44c5-9710-591f40f5e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_scylladb(batch_df, batch_id):\n",
    "    try:\n",
    "        print(f\"Writing batch {batch_id} to ScyllaDB\")\n",
    "        batch_df.write \\\n",
    "            .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "            .options(keyspace=\"keyspace_dev\", table=\"fin_trans_table\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "        print(f\"Batch {batch_id} written successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing batch {batch_id} to ScyllaDB: {e}\")\n",
    "        raise e  # Fail the stream if the batch fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "243e93f1-46d7-49cc-ba0a-c50708a6f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write micro-batches to ClickHouse\n",
    "def write_to_clickhouse(batch_df, batch_id):\n",
    "    try:\n",
    "        print(f\"Writing batch {batch_id} to ClickHouse\")\n",
    "        batch_df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:clickhouse://clickhouse:8123/testdb1\") \\\n",
    "            .option(\"dbtable\", \"testdb1.fin_trans_table\") \\\n",
    "            .option(\"user\", \"click\") \\\n",
    "            .option(\"password\", \"click\") \\\n",
    "            .option(\"driver\", \"ru.yandex.clickhouse.ClickHouseDriver\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "        print(f\"Batch {batch_id} written successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing batch {batch_id} to ClickHouse: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ba932-50b2-4097-bb67-77a0eb2c7ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- kafka_key: string (nullable = true)\n",
      " |-- kafka_value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark\\\n",
    "      .readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "      .option(\"subscribe\", \"fin_trans_topic\") \\\n",
    "      .option(\"startingOffsets\", \"earliest\") \\\n",
    "      .option(\"failOnDataLoss\", \"false\") \\\n",
    "      .load()\n",
    "      \n",
    "'''\n",
    "query = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"checkpointLocation\", \"/home/jovyan/work/book_data/\") \\\n",
    "    .start()\n",
    "'''\n",
    "\n",
    "# Select and cast key and value columns to string\n",
    "query_df = df.selectExpr(\"CAST(key AS STRING) AS kafka_key\", \"CAST(value AS STRING) AS kafka_value\")\n",
    "\n",
    "query_df.printSchema()\n",
    "\n",
    "\n",
    "# Write the streaming data to PostgreSQL using foreachBatch\n",
    "query = query_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(write_to_clickhouse) \\\n",
    "    .option(\"checkpointLocation\", \"/home/jovyan/work/book_data/\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a856aa5-b42c-4c9f-b4a1-2c7fdd47c19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1524a30-991f-4766-9f40-4e378cc83986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
